{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input values:\n",
      " [ 0.05  0.1 ]\n",
      "initial HIDDEN layer weights:\n",
      " [[ 0.15  0.2 ]\n",
      " [ 0.25  0.3 ]]\n",
      "initial HIDDEN layer bias weights:\n",
      " [ 0.35  0.35]\n",
      "initial OUTPUT layer weights:\n",
      " [[ 0.4   0.45]\n",
      " [ 0.5   0.55]]\n",
      "initial OUTPUT layer bias weights:\n",
      " [ 0.6  0.6]\n",
      "========\n",
      "==============\n",
      "==============\n",
      "\n",
      "Output after training with 10000 runs:\n",
      "Total error of the neural network after each run:\n",
      " [  2.98371109e-01   1.34911767e-02   5.74197301e-03   3.47777194e-03\n",
      "   2.43212527e-03   1.83954926e-03   1.46180009e-03   1.20179705e-03\n",
      "   1.01289869e-03   8.70034480e-04   7.58579749e-04   6.69453801e-04\n",
      "   5.96733501e-04   5.36397991e-04   4.85626153e-04   4.42382898e-04\n",
      "   4.05164981e-04   3.72839070e-04   3.44535347e-04   3.19575681e-04\n",
      "   2.97423979e-04   2.77651134e-04   2.59909821e-04   2.43916062e-04\n",
      "   2.29435536e-04   2.16273278e-04   2.04265818e-04   1.93275137e-04\n",
      "   1.83183936e-04   1.73891936e-04   1.65312923e-04   1.57372395e-04\n",
      "   1.50005658e-04   1.43156280e-04   1.36774831e-04   1.30817836e-04\n",
      "   1.25246920e-04   1.20028086e-04   1.15131120e-04   1.10529081e-04\n",
      "   1.06197877e-04   1.02115904e-04   9.82637347e-05   9.46238553e-05\n",
      "   9.11804399e-05   8.79191533e-05   8.48269829e-05   8.18920909e-05\n",
      "   7.91036871e-05   7.64519171e-05   7.39277640e-05   7.15229635e-05\n",
      "   6.92299271e-05   6.70416764e-05   6.49517832e-05   6.29543178e-05\n",
      "   6.10438019e-05   5.92151673e-05   5.74637188e-05   5.57851014e-05\n",
      "   5.41752700e-05   5.26304631e-05   5.11471788e-05   4.97221533e-05\n",
      "   4.83523410e-05   4.70348970e-05   4.57671615e-05   4.45466448e-05\n",
      "   4.33710143e-05   4.22380827e-05   4.11457969e-05   4.00922281e-05\n",
      "   3.90755627e-05   3.80940942e-05   3.71462150e-05   3.62304102e-05\n",
      "   3.53452508e-05   3.44893875e-05   3.36615462e-05   3.28605219e-05\n",
      "   3.20851749e-05   3.13344264e-05   3.06072544e-05   2.99026901e-05\n",
      "   2.92198148e-05   2.85577564e-05   2.79156869e-05   2.72928197e-05\n",
      "   2.66884066e-05   2.61017363e-05   2.55321316e-05   2.49789477e-05\n",
      "   2.44415703e-05   2.39194137e-05   2.34119195e-05   2.29185549e-05\n",
      "   2.24388111e-05   2.19722024e-05   2.15182645e-05   2.10765539e-05\n",
      "   2.06466460e-05]\n",
      "Targets:\n",
      " [ 0.01  0.99]\n",
      "Final neural network outputs:\n",
      " [ 0.01454408  0.98545636]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following multi-layer perceptron builds heavily on two sources:\n",
    "Blog: A Step by Step Backpropagation Example - https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "Book: Machine learning, An algorithmic perspective 2nd Edition by Stephen Marsland\n",
    "\n",
    "At the end is the run section, run it with the default values from the blog. No external files needed to run.\n",
    "\"\"\"\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class neural_network:\n",
    "    def __init__(self, weights_hidden, weights_output, bias_hidden_weight, bias_output_weight, n_hidden_layers=2, learning_rate=0.5):\n",
    "        self.weights_hidden = weights_hidden\n",
    "        self.weights_output = weights_output\n",
    "        self.bias_hidden_weight = np.array([bias_hidden_weight] * 2)\n",
    "        self.bias_output_weight = np.array([bias_output_weight] * 2)\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = 1\n",
    "        \n",
    "        self.hidden_layer_activation = np.array([0.,0.])\n",
    "        self.hidden_layer_activation_f_output = np.array([0.,0.])\n",
    "        self.hidden_layer_error = np.array([0.,0.])\n",
    "        self.output_layer_output = np.array([0.,0.])\n",
    "        self.output_layer_activation = np.array([0.,0.])\n",
    "        self.output_layer_error = np.array([0., 0.])\n",
    "        \n",
    "        self.all_total_errors = np.array([])\n",
    "    \n",
    "    #activation function - sigmoid is acceptable when having a classification problem\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + math.exp(-x * self.beta))\n",
    "\n",
    "    ##calculate hidden layer activations and outputs from activation function\n",
    "    def forward_hidden_layer(self): \n",
    "        for i in range(0, self.n_hidden_layers):\n",
    "            curr_weights = self.weights_hidden[i,:]\n",
    "            \n",
    "            #calculate hidden neuron value\n",
    "            current_hidden_neutron = np.dot(self.inputs, curr_weights) + self.bias_hidden_weight[i]\n",
    "            np.put(self.hidden_layer_activation, i, current_hidden_neutron)\n",
    "            np.put(self.hidden_layer_activation_f_output, i, self.sigmoid(current_hidden_neutron))\n",
    "\n",
    "    ##calculate output layer\n",
    "    def forward_output_layer(self):\n",
    "        n_target_neurons = self.targets.shape[0] #get number og output neurons (elements in target)\n",
    "        for i in range(0, n_target_neurons):\n",
    "            #print(\"--- Calculate output neuron number:\", i)\n",
    "            curr_weights = self.weights_output[i,:]\n",
    "            #calculate output neuron value\n",
    "            current_output_neutron = np.dot(self.hidden_layer_activation_f_output, curr_weights) + self.bias_output_weight[i]\n",
    "            np.put(self.output_layer_output, i, current_output_neutron)\n",
    "            np.put(self.output_layer_activation, i, self.sigmoid(current_output_neutron))\n",
    "    \n",
    "    def calculate_total_error(self, counter):\n",
    "        total_error = np.sum(1/2 * (self.targets - self.output_layer_activation) ** 2)\n",
    "        if (counter % 100 == 0 or counter == 1): #add only every 100th neural network error to the array\n",
    "            self.all_total_errors = np.append(self.all_total_errors, total_error)\n",
    "    \n",
    "    def calculate_output_error(self):\n",
    "        ##compute the error at the output\n",
    "        self.output_layer_error = (self.output_layer_activation - self.targets) * self.output_layer_activation * (1 - self.output_layer_activation) \n",
    "         \n",
    "    ##calculate the error in the hidden layer\n",
    "    def backward_hidden_error(self):\n",
    "        #first part of the equation - the derivative of the activation function - (x*(1-x))\n",
    "        derivative_part = self.hidden_layer_activation_f_output * (1 - self.hidden_layer_activation_f_output)\n",
    "        #second part of the equation - sum of product of weights in hidden layer and the error of the output\n",
    "        sum_part = self.weights_hidden * self.output_layer_error        \n",
    "        self.hidden_layer_error = derivative_part * sum_part\n",
    "        \n",
    "    def update_output_layer_weights(self):\n",
    "        self.weights_output = self.weights_output - (self.learning_rate * self.output_layer_error * self.hidden_layer_activation_f_output)\n",
    "        self.bias_output_weight = self.bias_output_weight - (self.learning_rate * self.output_layer_error * 1)\n",
    "        \n",
    "    def update_hidden_layer_weights(self):\n",
    "        self.weights_hidden =  self.weights_hidden - (self.learning_rate * self.hidden_layer_error * self.inputs)\n",
    "        self.bias_hidden_weight = self.bias_hidden_weight - (self.learning_rate * self.hidden_layer_activation * 1)\n",
    "    \n",
    "    ##train method\n",
    "    def train(self, inputs, targets, n_runs):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.n_runs = n_runs\n",
    "        \n",
    "        print(\"input values:\\n\", self.inputs)\n",
    "        print(\"initial HIDDEN layer weights:\\n\", self.weights_hidden)\n",
    "        print(\"initial HIDDEN layer bias weights:\\n\", self.bias_hidden_weight)\n",
    "        print(\"initial OUTPUT layer weights:\\n\", self.weights_output)\n",
    "        print(\"initial OUTPUT layer bias weights:\\n\", self.bias_output_weight)\n",
    "        print(\"========\") \n",
    "        \n",
    "        for i in range(1, n_runs + 1):\n",
    "            ##################\n",
    "            ## Forward pass #\n",
    "            ################\n",
    "            self.forward_hidden_layer()\n",
    "            self.forward_output_layer()      \n",
    "            self.calculate_total_error(i)\n",
    "            \n",
    "            ###################\n",
    "            ## Backward pass #\n",
    "            #################\n",
    "            self.calculate_output_error()\n",
    "            self.backward_hidden_error()\n",
    "            \n",
    "            self.update_output_layer_weights()\n",
    "            self.update_hidden_layer_weights() \n",
    "        \n",
    "    #end train    \n",
    "    \n",
    "    ##print an output with final information about the learning of neural network\n",
    "    def get_final_output(self):\n",
    "        print(\"==============\\n==============\")\n",
    "        print(\"\\nOutput after training with {} runs:\".format(self.n_runs))\n",
    "        print(\"Total error of the neural network after each run:\\n\", self.all_total_errors)\n",
    "        print(\"Targets:\\n\", self.targets)\n",
    "        print(\"Final neural network outputs:\\n\", self.output_layer_activation)\n",
    "    \n",
    "#end class neural_network        \n",
    "\n",
    "\n",
    "#########\n",
    "## Run #\n",
    "#######\n",
    "\n",
    "#original from the blog\n",
    "X = np.array([0.05, 0.1])\n",
    "target= np.array([0.01, 0.99])\n",
    "\n",
    "weights_hidden_layer = np.array([[0.15, 0.2], [0.25, 0.3]])\n",
    "weights_output_layer = np.array([[0.4, 0.45], [0.5, 0.55]])\n",
    "bias_hidden_weight = 0.35\n",
    "bias_output_weight = 0.6\n",
    "number_of_training_runs = 10000\n",
    "\n",
    "nn = neural_network(weights_hidden_layer, weights_output_layer, bias_hidden_weight, bias_output_weight, learning_rate=0.5)\n",
    "nn.train(X, target, number_of_training_runs)\n",
    "nn.get_final_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
